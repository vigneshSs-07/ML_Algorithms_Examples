The important assumptions in regression analysis:

   -> There should be a linear and additive relationship between dependent (response) variable and independent (predictor) variable(s). 
       A linear relationship suggests that a change in response Y due to one unit change in X¹ is constant, regardless of the value of X¹. 
       An additive relationship suggests that the effect of X¹ on Y is independent of other variables.
   -> There should be no correlation between the residual (error) terms. Absence of this phenomenon is known as Autocorrelation.
   -> The independent variables should not be correlated. Absence of this phenomenon is known as multicollinearity.
   -> The error terms must have constant variance. This phenomenon is known as homoskedasticity. The presence of non-constant variance is referred to heteroskedasticity.
   -> The error terms must be normally distributed.

Types of Regressions :

   -> Linear Regression
   -> Logistic Regression
   -> Polynomial Regression
   -> Stepwise Regression
   -> Ridge Regression
   -> Lasso Regression
   -> ElasticNet Regression

Important Points:

    -> There must be linear relationship between independent and dependent variables
    -> Multiple regression suffers from multicollinearity, autocorrelation, heteroskedasticity.
    -> Linear Regression is very sensitive to Outliers. It can terribly affect the regression line and eventually the forecasted values.
    -> Multicollinearity can increase the variance of the coefficient estimates and make the estimates very sensitive to minor changes in the model.
       The result is that the coefficient estimates are unstable
    -> In case of multiple independent variables, we can go with forward selection, backward elimination and step wise approach for selection of most significant independent variables.



